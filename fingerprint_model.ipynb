{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "# %reset"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<h2>Pre Process Dataset</h2>\n",
    "<h6>get dataset</br>\n",
    "split train and test data</br>\n",
    "spilt every matrix song to one second segment</br></h6>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "(13281,)"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "DATASET_PATH = \"./all.npy\"\n",
    "with open(DATASET_PATH, 'rb') as f:\n",
    "    data = np.load(f, allow_pickle=True)\n",
    "data.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "((10624,), (2657,), (1995, 12))"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data_train, data_test, _, _ = train_test_split(data, data, test_size=0.2, random_state=42)\n",
    "data_train.shape, data_test.shape, data_train[0].shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "import librosa\n",
    "import tensorflow as tf\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "sample_rate = 16000\n",
    "hop_length = int(sample_rate * 0.1)\n",
    "n_fft = int(sample_rate * 0.2)\n",
    "DIFF = 1\n",
    "allowed_duration = 10000\n",
    "special_value = -10\n",
    "frame_sec_indexes = [0, ] + [librosa.time_to_frames(i, sample_rate, n_fft=n_fft, hop_length=hop_length)\n",
    "                             for i in range(1, allowed_duration, DIFF)]\n",
    "\n",
    "max_frames_in_diff=max([frame_sec_indexes[i+1] - frame_sec_indexes[i] for i in range(len(frame_sec_indexes)-1)])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "def clean_frame_matrix(feature, start:int, end:int):\n",
    "    return_matrix = feature[start: end].tolist()\n",
    "    # add padding\n",
    "    return_matrix += [[special_value for j in range(12)]for k in range(max_frames_in_diff-len(return_matrix))]\n",
    "    return return_matrix\n",
    "\n",
    "def split_features(features):\n",
    "\n",
    "    split_features = list()\n",
    "    # song_indexes = dict()\n",
    "    for i in range(len(features)):\n",
    "        pre_len = len(split_features)\n",
    "        if (len(features[i])) > frame_sec_indexes[-1]:\n",
    "            print(len(features[i]), i)\n",
    "        split_feature_i = [clean_frame_matrix(features[i], frame_sec_indexes[index], frame_sec_indexes[index+1])\n",
    "                           for index in range(len(frame_sec_indexes)-1)\n",
    "                           if frame_sec_indexes[index] < len(features[i])]\n",
    "        # add data to split_feature\n",
    "        split_features += split_feature_i\n",
    "\n",
    "        # add data to song_indexes {song3: (start_index, end_index), }\n",
    "        # song_indexes[i] = (pre_len, len(split_features))\n",
    "\n",
    "    return np.array(split_features)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data_train = split_features(data_train)\n",
    "data_test = split_features(data_test)\n",
    "data_train.shape, data_test.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<h2>seq2seq model</h2>\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "outputs": [],
   "source": [
    "from keras.layers import LSTM, GRU, Dense, Input, RepeatVector, TimeDistributed, Masking\n",
    "from keras.models import Model\n",
    "\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "encoder_inputs = Input(shape=(None, 12))\n",
    "masking = Masking(mask_value=special_value)(encoder_inputs)\n",
    "encoder = LSTM(128, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder(masking)\n",
    "encoder_states = [state_h, state_c]\n",
    "# define training decoder\n",
    "decoder_inputs = RepeatVector(12)(encoder_outputs)\n",
    "\n",
    "# decoder_inputs = Input(shape=(None, 12))\n",
    "decoder_lstm = LSTM(128, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_inputs, initial_state=encoder_states)\n",
    "decoder_time = TimeDistributed(Dense(12, activation='softmax'))\n",
    "decoder_outputs = decoder_time(decoder_outputs)\n",
    "model = Model(encoder_inputs, decoder_outputs)\n",
    "# define inference encoder\n",
    "encoder_model = Model(encoder_inputs, encoder_outputs)\n",
    "# define inference decoder\n",
    "decoder_state_input_h = Input(shape=(128,))\n",
    "decoder_state_input_c = Input(shape=(128,))\n",
    "decoder_inputs_layer = Input(shape=(12, 128))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "decoder_outputs, state_h, state_c = decoder_lstm(decoder_inputs_layer, initial_state=decoder_states_inputs)\n",
    "decoder_states = [state_h, state_c]\n",
    "decoder_outputs = decoder_time(decoder_outputs)\n",
    "decoder_model = Model([decoder_inputs_layer] + decoder_states_inputs, [decoder_outputs] + decoder_states)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Can't convert non-rectangular Python sequence to Tensor.",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp/ipykernel_14264/976166919.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[0mmodel\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mcompile\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0moptimizer\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mtf\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mkeras\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0moptimizers\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mAdam\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mloss\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mtf\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mkeras\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mlosses\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mHuber\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 2\u001B[1;33m \u001B[0mhistory\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mmodel\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mfit\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtf\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mconvert_to_tensor\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mfinal_train_data\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtf\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mconvert_to_tensor\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mfinal_train_data\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mepochs\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;36m20\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      3\u001B[0m \u001B[0mhistory\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\Users\\MJSH\\Documents\\GitHub\\AudioFingerprintModel\\venv\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py\u001B[0m in \u001B[0;36mwrapper\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    204\u001B[0m     \u001B[1;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    205\u001B[0m     \u001B[1;32mtry\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 206\u001B[1;33m       \u001B[1;32mreturn\u001B[0m \u001B[0mtarget\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    207\u001B[0m     \u001B[1;32mexcept\u001B[0m \u001B[1;33m(\u001B[0m\u001B[0mTypeError\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mValueError\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    208\u001B[0m       \u001B[1;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\Users\\MJSH\\Documents\\GitHub\\AudioFingerprintModel\\venv\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001B[0m in \u001B[0;36mconvert_to_tensor_v2_with_dispatch\u001B[1;34m(value, dtype, dtype_hint, name)\u001B[0m\n\u001B[0;32m   1428\u001B[0m     \u001B[0mValueError\u001B[0m\u001B[1;33m:\u001B[0m \u001B[0mIf\u001B[0m \u001B[0mthe\u001B[0m\u001B[0;31m \u001B[0m\u001B[0;31m`\u001B[0m\u001B[0mvalue\u001B[0m\u001B[0;31m`\u001B[0m \u001B[1;32mis\u001B[0m \u001B[0ma\u001B[0m \u001B[0mtensor\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[0mof\u001B[0m \u001B[0mgiven\u001B[0m\u001B[0;31m \u001B[0m\u001B[0;31m`\u001B[0m\u001B[0mdtype\u001B[0m\u001B[0;31m`\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mgraph\u001B[0m \u001B[0mmode\u001B[0m\u001B[1;33m.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1429\u001B[0m   \"\"\"\n\u001B[1;32m-> 1430\u001B[1;33m   return convert_to_tensor_v2(\n\u001B[0m\u001B[0;32m   1431\u001B[0m       value, dtype=dtype, dtype_hint=dtype_hint, name=name)\n\u001B[0;32m   1432\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\Users\\MJSH\\Documents\\GitHub\\AudioFingerprintModel\\venv\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001B[0m in \u001B[0;36mconvert_to_tensor_v2\u001B[1;34m(value, dtype, dtype_hint, name)\u001B[0m\n\u001B[0;32m   1434\u001B[0m \u001B[1;32mdef\u001B[0m \u001B[0mconvert_to_tensor_v2\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mvalue\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mdtype\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;32mNone\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mdtype_hint\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;32mNone\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mname\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;32mNone\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1435\u001B[0m   \u001B[1;34m\"\"\"Converts the given `value` to a `Tensor`.\"\"\"\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1436\u001B[1;33m   return convert_to_tensor(\n\u001B[0m\u001B[0;32m   1437\u001B[0m       \u001B[0mvalue\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mvalue\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1438\u001B[0m       \u001B[0mdtype\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mdtype\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\Users\\MJSH\\Documents\\GitHub\\AudioFingerprintModel\\venv\\lib\\site-packages\\tensorflow\\python\\profiler\\trace.py\u001B[0m in \u001B[0;36mwrapped\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    161\u001B[0m         \u001B[1;32mwith\u001B[0m \u001B[0mTrace\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtrace_name\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mtrace_kwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    162\u001B[0m           \u001B[1;32mreturn\u001B[0m \u001B[0mfunc\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 163\u001B[1;33m       \u001B[1;32mreturn\u001B[0m \u001B[0mfunc\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    164\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    165\u001B[0m     \u001B[1;32mreturn\u001B[0m \u001B[0mwrapped\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\Users\\MJSH\\Documents\\GitHub\\AudioFingerprintModel\\venv\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001B[0m in \u001B[0;36mconvert_to_tensor\u001B[1;34m(value, dtype, name, as_ref, preferred_dtype, dtype_hint, ctx, accepted_result_types)\u001B[0m\n\u001B[0;32m   1564\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1565\u001B[0m     \u001B[1;32mif\u001B[0m \u001B[0mret\u001B[0m \u001B[1;32mis\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1566\u001B[1;33m       \u001B[0mret\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mconversion_func\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mvalue\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mdtype\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mdtype\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mname\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mname\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mas_ref\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mas_ref\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1567\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1568\u001B[0m     \u001B[1;32mif\u001B[0m \u001B[0mret\u001B[0m \u001B[1;32mis\u001B[0m \u001B[0mNotImplemented\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\Users\\MJSH\\Documents\\GitHub\\AudioFingerprintModel\\venv\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py\u001B[0m in \u001B[0;36m_constant_tensor_conversion_function\u001B[1;34m(v, dtype, name, as_ref)\u001B[0m\n\u001B[0;32m    344\u001B[0m                                          as_ref=False):\n\u001B[0;32m    345\u001B[0m   \u001B[0m_\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mas_ref\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 346\u001B[1;33m   \u001B[1;32mreturn\u001B[0m \u001B[0mconstant\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mv\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mdtype\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mdtype\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mname\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mname\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    347\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    348\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\Users\\MJSH\\Documents\\GitHub\\AudioFingerprintModel\\venv\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py\u001B[0m in \u001B[0;36mconstant\u001B[1;34m(value, dtype, shape, name)\u001B[0m\n\u001B[0;32m    269\u001B[0m     \u001B[0mValueError\u001B[0m\u001B[1;33m:\u001B[0m \u001B[1;32mif\u001B[0m \u001B[0mcalled\u001B[0m \u001B[0mon\u001B[0m \u001B[0ma\u001B[0m \u001B[0msymbolic\u001B[0m \u001B[0mtensor\u001B[0m\u001B[1;33m.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    270\u001B[0m   \"\"\"\n\u001B[1;32m--> 271\u001B[1;33m   return _constant_impl(value, dtype, shape, name, verify_shape=False,\n\u001B[0m\u001B[0;32m    272\u001B[0m                         allow_broadcast=True)\n\u001B[0;32m    273\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\Users\\MJSH\\Documents\\GitHub\\AudioFingerprintModel\\venv\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py\u001B[0m in \u001B[0;36m_constant_impl\u001B[1;34m(value, dtype, shape, name, verify_shape, allow_broadcast)\u001B[0m\n\u001B[0;32m    281\u001B[0m       \u001B[1;32mwith\u001B[0m \u001B[0mtrace\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mTrace\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m\"tf.constant\"\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    282\u001B[0m         \u001B[1;32mreturn\u001B[0m \u001B[0m_constant_eager_impl\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mctx\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mvalue\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mdtype\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mshape\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mverify_shape\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 283\u001B[1;33m     \u001B[1;32mreturn\u001B[0m \u001B[0m_constant_eager_impl\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mctx\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mvalue\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mdtype\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mshape\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mverify_shape\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    284\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    285\u001B[0m   \u001B[0mg\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mops\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mget_default_graph\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\Users\\MJSH\\Documents\\GitHub\\AudioFingerprintModel\\venv\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py\u001B[0m in \u001B[0;36m_constant_eager_impl\u001B[1;34m(ctx, value, dtype, shape, verify_shape)\u001B[0m\n\u001B[0;32m    306\u001B[0m \u001B[1;32mdef\u001B[0m \u001B[0m_constant_eager_impl\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mctx\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mvalue\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mdtype\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mshape\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mverify_shape\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    307\u001B[0m   \u001B[1;34m\"\"\"Creates a constant on the current device.\"\"\"\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 308\u001B[1;33m   \u001B[0mt\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mconvert_to_eager_tensor\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mvalue\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mctx\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mdtype\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    309\u001B[0m   \u001B[1;32mif\u001B[0m \u001B[0mshape\u001B[0m \u001B[1;32mis\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    310\u001B[0m     \u001B[1;32mreturn\u001B[0m \u001B[0mt\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\Users\\MJSH\\Documents\\GitHub\\AudioFingerprintModel\\venv\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py\u001B[0m in \u001B[0;36mconvert_to_eager_tensor\u001B[1;34m(value, ctx, dtype)\u001B[0m\n\u001B[0;32m    104\u001B[0m       \u001B[0mdtype\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mdtypes\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mas_dtype\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mdtype\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mas_datatype_enum\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    105\u001B[0m   \u001B[0mctx\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mensure_initialized\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 106\u001B[1;33m   \u001B[1;32mreturn\u001B[0m \u001B[0mops\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mEagerTensor\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mvalue\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mctx\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdevice_name\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mdtype\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    107\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    108\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mValueError\u001B[0m: Can't convert non-rectangular Python sequence to Tensor."
     ]
    }
   ],
   "source": [
    "model.compile(optimizer=tf.keras.optimizers.Adam(), loss=tf.keras.losses.Huber())\n",
    "history = model.fit(data_train, data_train, epochs=20)\n",
    "history"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}