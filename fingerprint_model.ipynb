{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "fingerprint_model.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "AHzHdrSKJQQi"
      },
      "source": [
        "# %reset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GqYtCZWNJU5k",
        "outputId": "fa9b59ce-0e65-4623-a9f9-81de0b40c3c6"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "zhhfhN6MJQQq"
      },
      "source": [
        "<h2>Pre Process Dataset</h2>\n",
        "<h6>get dataset</br>\n",
        "split train and test data</br>\n",
        "spilt every matrix song to one second segment</br></h6>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EMHMPubbJQQu"
      },
      "source": [
        "import numpy as np\n",
        "import os\n",
        "\n",
        "DATASET_PATH = \"./drive/MyDrive/chromagram_dataset/data.npy\"\n",
        "DATASET_TRAIN = \"./drive/MyDrive/chromagram_dataset/train.npy\"\n",
        "DATASET_TEST = \"./drive/MyDrive/chromagram_dataset/test.npy\"\n",
        "saved_data = os.path.exists(DATASET_TRAIN) and os.path.exists(DATASET_TEST)\n",
        "if not saved_data:\n",
        "  with open(DATASET_PATH, 'rb') as f:\n",
        "      data = np.load(f, allow_pickle=True)\n",
        "  data.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PEqsSDOBJQQz"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "if not saved_data:\n",
        "  data_train, data_test, _, _ = train_test_split(data, data, test_size=0.3, random_state=42)\n",
        "  del data\n",
        "  data_train.shape, data_test.shape, data_train[0].shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zFVGucxlJQQ2"
      },
      "source": [
        "import librosa\n",
        "import tensorflow as tf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wy5Q5xppJQQ4"
      },
      "source": [
        "sample_rate = 16000\n",
        "hop_length = int(sample_rate * 0.1)\n",
        "n_fft = int(sample_rate * 0.2)\n",
        "DIFF = 1\n",
        "allowed_duration = 10000\n",
        "special_value = -0.1\n",
        "frame_sec_indexes = [librosa.time_to_frames(i, sample_rate, n_fft=n_fft, hop_length=hop_length)\n",
        "                     for i in range(1, allowed_duration, DIFF)]\n",
        "\n",
        "max_frames_in_diff=max([frame_sec_indexes[i+1] - frame_sec_indexes[i] for i in range(len(frame_sec_indexes)-1)])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yRjeg7dmJQQ6"
      },
      "source": [
        "def clean_frame_matrix(feature: np.array):\n",
        "    # add padding\n",
        "    if 0 in feature.shape:\n",
        "      print('here')\n",
        "    full_matrix = np.full((max_frames_in_diff, 12), special_value, dtype=np.float32)\n",
        "    full_matrix[:feature.shape[0], :feature.shape[1]] = feature\n",
        "\n",
        "    return full_matrix\n",
        "\n",
        "def split_features(features):\n",
        "\n",
        "    split_features = np.empty((0, 10, 12))\n",
        "    # song_indexes = dict()\n",
        "    for i in range(len(features)):\n",
        "        # pre_len = len(split_features)\n",
        "        split_feature = np.split(features[i], [each for each in frame_sec_indexes if each < len(features[i])])\n",
        "        split_features = np.append(split_features, np.array([clean_frame_matrix(each) for each in split_feature]),\n",
        "                                   axis=0)\n",
        "        print(f'\\r{i} done', end='\\r')\n",
        "\n",
        "        # add data to song_indexes {song3: (start_index, end_index), }\n",
        "        # song_indexes[i] = (pre_len, len(split_features))\n",
        "\n",
        "    return split_features\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8NpXe1THJQQ_"
      },
      "source": [
        "if not saved_data:\n",
        "  data_train = split_features(data_train)\n",
        "  with open(DATASET_TRAIN, 'wb') as f:\n",
        "    np.save(f, data_train)\n",
        "  print(data_train.shape)\n",
        "else: \n",
        "  with open(DATASET_TRAIN, 'rb') as f:\n",
        "      data_train = np.load(f, allow_pickle=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i8wUHiigJQRC"
      },
      "source": [
        "if not saved_data:\n",
        "  data_test = split_features(data_test)\n",
        "  with open(DATASET_TEST, 'wb') as f:\n",
        "    np.save(f, data_test)\n",
        "  print(data_test.shape)\n",
        "else: \n",
        "  with open(DATASET_TEST, 'rb') as f:\n",
        "      data_test = np.load(f, allow_pickle=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "_2VwFLKiJQRJ"
      },
      "source": [
        "<h2>seq2seq model</h2>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jjmFv_k2ZZQ8"
      },
      "source": [
        "from keras.layers import LSTM, GRU, Dense, Input, RepeatVector, TimeDistributed, Masking, Activation\n",
        "from keras.models import Model\n",
        "from keras import backend as K\n",
        "from keras.utils.generic_utils import get_custom_objects\n",
        "\n",
        "# for recreate model\n",
        "tf.keras.backend.clear_session()\n",
        "\n",
        "def binary_activation(x):\n",
        "  return K.cast(K.greater(x, 0), K.floatx())\n",
        "\n",
        "get_custom_objects().update({'binary_activation': Activation(binary_activation)})\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DDxYEHg9JQRK"
      },
      "source": [
        "\n",
        "# encoder layers\n",
        "encoder_inputs = Input(shape=(int(max_frames_in_diff), 12))\n",
        "masking = Masking(mask_value=np.float32(special_value))(encoder_inputs)\n",
        "encoder_lstm = LSTM(300, return_state=True, return_sequences=True)\n",
        "encoder_outputs, state_h, state_c = encoder_lstm(masking)\n",
        "encoder_lstm2 = LSTM(24, return_state=True)\n",
        "encoder_outputs2, state_h2, state_c2 = encoder_lstm2(encoder_outputs)\n",
        "activation_layer = Activation(binary_activation)\n",
        "encoder_outputs2 = activation_layer(encoder_outputs2)\n",
        "encoder_states = [state_h, state_c]\n",
        "encoder_states2 = [state_h2, state_c2]\n",
        "\n",
        "# decoder input\n",
        "# decoder_inputs = RepeatVector(int(max_frames_in_diff))(encoder_outputs2)\n",
        "decoder_input = Input(shape=(int(max_frames_in_diff), 12))\n",
        "decoder_masking = Masking(mask_value=np.float32(special_value))(decoder_input)\n",
        "\n",
        "# decoder layers\n",
        "decoder_lstm = LSTM(24, return_sequences=True, return_state=True)\n",
        "decoder_outputs, _, _ = decoder_lstm(decoder_masking, initial_state=encoder_states2)\n",
        "decoder_lstm2 = LSTM(300, return_sequences=True, return_state=True)\n",
        "decoder_outputs2, _, _ = decoder_lstm2(decoder_outputs, initial_state=encoder_states)\n",
        "decoder_time = TimeDistributed(Dense(12, activation='softmax'))\n",
        "decoder_outputs2 = decoder_time(decoder_outputs2)\n",
        "\n",
        "# define model\n",
        "model = Model([encoder_inputs, decoder_input], decoder_outputs2)\n",
        "\n",
        "# define encoder model\n",
        "encoder_model = Model(encoder_inputs, encoder_outputs2)\n",
        "\n",
        "# define inference decoder\n",
        "decoder_state_input_h, decoder_state_input_c = Input(shape=(300,)), Input(shape=(300,))\n",
        "decoder_state_input_h2, decoder_state_input_c2 = Input(shape=(24,)), Input(shape=(24,))\n",
        "decoder_inputs_layer = Input(shape=(int(max_frames_in_diff), 24))\n",
        "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c, \n",
        "                         decoder_state_input_h2, decoder_state_input_c2]\n",
        "decoder_outputs, state_h, state_c = decoder_lstm2(decoder_inputs_layer, initial_state=decoder_states_inputs[:2])\n",
        "decoder_states = [state_h, state_c]\n",
        "decoder_outputs = decoder_time(decoder_outputs)\n",
        "decoder_model = Model([decoder_inputs_layer] + decoder_states_inputs, [decoder_outputs] + decoder_states)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q_4W-eA0JQRM"
      },
      "source": [
        "model.compile(optimizer=tf.keras.optimizers.Adam(), loss=\"mse\", metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SB90DzKKJQRO"
      },
      "source": [
        "model.fit([data_train, data_train], data_train, epochs=40, validation_split=0.2, batch_size=512)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "141Fvc4DaTPT"
      },
      "source": [
        "model.evaluate([data_test, data_test], data_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WLUOs4RwYipc"
      },
      "source": [
        "# save model\n",
        "model_json = model.to_json()\n",
        "with open(\"./drive/MyDrive/fingerprint_model/fingerprint_model.json\", \"w\") as json_file:\n",
        "    json_file.write(model_json)\n",
        "# serialize weights to HDF5\n",
        "model.save_weights(\"./drive/MyDrive/fingerprint_model/fingerprint_model.h5\")\n",
        "\n",
        "# save encoder and decoder\n",
        "encoder_json = encoder_model.to_json()\n",
        "with open(\"./drive/MyDrive/fingerprint_model/encoder_model.json\", \"w\") as json_file:\n",
        "    json_file.write(encoder_json)\n",
        "encoder_model.save_weights(\"./drive/MyDrive/fingerprint_model/encoder_model.h5\")\n",
        "\n",
        "decoder_json = decoder_model.to_json()\n",
        "with open(\"./drive/MyDrive/fingerprint_model/decoder_model.json\", \"w\") as json_file:\n",
        "    json_file.write(decoder_json)\n",
        "decoder_model.save_weights(\"./drive/MyDrive/fingerprint_model/decoder_model.h5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0VpSTJS8A-lW"
      },
      "source": [
        "# load encoder\n",
        "from keras.models import model_from_json\n",
        "json_file = open('./drive/MyDrive/fingerprint_model/encoder_model.json', 'r')\n",
        "loaded_model_json = json_file.read()\n",
        "json_file.close()\n",
        "loaded_model = model_from_json(loaded_model_json, \n",
        "                               {'binary_activation': Activation(binary_activation)})\n",
        "# load weights into new model\n",
        "loaded_model.load_weights(\"./drive/MyDrive/fingerprint_model/encoder_model.h5\")\n",
        "print(\"Loaded model from disk\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}